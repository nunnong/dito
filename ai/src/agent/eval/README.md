# Intervention Agent Evaluation System

## ğŸ“‹ ê°œìš”

Intervention Agentì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ê¸° ìœ„í•œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
eval/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ README.md                    # ì´ íŒŒì¼
â”œâ”€â”€ intervention_dataset.py      # Ground Truth ë°ì´í„°ì…‹ (20ê°œ Easy ì¼€ì´ìŠ¤)
â”œâ”€â”€ prompts.py                   # í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
â”œâ”€â”€ evaluate_intervention.py     # LangSmith ê¸°ë°˜ í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ results/                     # í‰ê°€ ê²°ê³¼ ì €ì¥ (gitì—ì„œ ì œì™¸)
    â””â”€â”€ .gitkeep
```

## ğŸ¯ í‰ê°€ ë©”íŠ¸ë¦­

1. **Trigger Event Accuracy**: íŠ¸ë¦¬ê±° ì´ë²¤íŠ¸ ë¶„ë¥˜ ì •í™•ë„ (ëª©í‘œ: 85%)
2. **Intervention Decision Accuracy**: ê°œì… í•„ìš”ì„± íŒë‹¨ ì •í™•ë„ (ëª©í‘œ: 80%)
3. **Severity In Range**: ì‹¬ê°ë„ ì ìˆ˜ ë²”ìœ„ ë‚´ ì •í™•ë„ (ëª©í‘œ: 80%)
4. **Severity MAE**: ì‹¬ê°ë„ ì ìˆ˜ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (ëª©í‘œ: < 1.5)
5. **Workflow Success Rate**: ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì„±ê³µë¥  (ëª©í‘œ: 95%)
6. **Nudge Frame Completeness**: ë„›ì§€ ë©”ì‹œì§€ í”„ë ˆì„ ì™„ì„±ë„ (ëª©í‘œ: 80%)

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
export LANGSMITH_API_KEY="your-api-key"
export LANGSMITH_PROJECT="intervention-agent-eval"
export LANGSMITH_TRACING_V2=true
export ANTHROPIC_API_KEY="your-anthropic-key"
```

### 2. LangSmith í‰ê°€ ì‹¤í–‰

```bash
cd ai
python src/agent/eval/evaluate_intervention.py

# ë³‘ë ¬ë„ ì¡°ì • (ê¸°ë³¸ê°’: 2)
python src/agent/eval/evaluate_intervention.py 4

# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜ ì œí•œ (ì²˜ìŒ 10ê°œë§Œ)
python src/agent/eval/evaluate_intervention.py 2 10
```

### 3. pytest ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
cd ai

# ëª¨ë“  ë…¸ë“œ í…ŒìŠ¤íŠ¸
pytest tests/test_intervention_nodes.py -v

# íŠ¹ì • í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest tests/test_intervention_nodes.py::test_analyze_behavior_trigger_event -v

# LangSmith ë¡œê¹… í¬í•¨
LANGSMITH_TRACING_V2=true pytest tests/test_intervention_nodes.py -v
```

## ğŸ“Š ê²°ê³¼ í™•ì¸

### LangSmith ëŒ€ì‹œë³´ë“œ
- URL: https://smith.langchain.com
- í”„ë¡œì íŠ¸: `intervention-agent-eval`
- ëª¨ë“  ì‹¤í–‰ ì¶”ì  ë° ìƒì„¸ ë¶„ì„ ê°€ëŠ¥

### ë¡œì»¬ ê²°ê³¼
- ê·¸ë˜í”„: `eval/results/evaluation_<timestamp>.png`
- ì½˜ì†” ì¶œë ¥: ê° ë©”íŠ¸ë¦­ë³„ ì ìˆ˜

## ğŸ“ ë°ì´í„°ì…‹ í™•ì¥

í˜„ì¬ 20ê°œ Easy ì¼€ì´ìŠ¤ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ ì¼€ì´ìŠ¤ë¥¼ ì‘ì„±í•˜ë ¤ë©´:

```python
# intervention_dataset.pyì— ì¶”ê°€

medium_case_1 = {
    "name": "M1_boundary_case",
    "input": {
        "user_id": 21,
        "behavior_log": {
            "app_name": "YouTube Shorts",
            "duration_seconds": 1200,  # ì •í™•íˆ 20ë¶„ (ê²½ê³„ê°’)
            "usage_timestamp": "2025-01-03T14:00:00",
            "recent_app_switches": 5
        }
    },
    "expected": {
        "trigger_event": ["short-form-overuse"],
        "pattern_type": ["concerning", "critical"],
        "severity_range": (5, 7),
        "intervention_needed": True,
        "intervention_type": ["short-form-overuse"],
        "urgency_level": ["medium"]
    },
    "nudge_criteria": {
        "must_mention_usage_time": True
    }
}

# medium_cases ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
medium_cases = [medium_case_1, ...]

# all_casesì— ë³‘í•©
all_cases = easy_cases + medium_cases
```

## ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ìƒˆ í‰ê°€ í•¨ìˆ˜ ì¶”ê°€

`evaluate_intervention.py`ì— evaluator í•¨ìˆ˜ë¥¼ ì¶”ê°€:

```python
def custom_evaluator(outputs: dict, reference_outputs: dict) -> dict:
    """ì»¤ìŠ¤í…€ í‰ê°€ ë¡œì§"""
    # í‰ê°€ ë¡œì§ êµ¬í˜„
    score = calculate_score(outputs, reference_outputs)

    return {
        "key": "custom_metric",
        "score": score
    }

# evaluators ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
evaluators = [
    trigger_event_accuracy_evaluator,
    # ...
    custom_evaluator,  # ì¶”ê°€
]
```

### ìƒˆ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€

`prompts.py`ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€:

```python
CUSTOM_EVALUATOR_PROMPT = """
<Task>
ì»¤ìŠ¤í…€ í‰ê°€ ê¸°ì¤€ ì„¤ëª…
</Task>

<input>{input_data}</input>
<output>{outputs}</output>
<reference>{reference_outputs}</reference>

í‰ê°€í•˜ì„¸ìš”.
"""
```

## ğŸ“ˆ ì„±ëŠ¥ ëª©í‘œ

| ë©”íŠ¸ë¦­ | í˜„ì¬ | ëª©í‘œ |
|--------|------|------|
| Trigger Event Accuracy | TBD | 85% |
| Intervention Decision Accuracy | TBD | 80% |
| Severity In Range | TBD | 80% |
| Severity MAE | TBD | < 1.5 |
| Workflow Success Rate | TBD | 95% |
| Nudge Frame Completeness | TBD | 80% |

## ğŸ› ë¬¸ì œ í•´ê²°

### LangSmith ì—°ê²° ì˜¤ë¥˜
```bash
# API í‚¤ í™•ì¸
echo $LANGSMITH_API_KEY

# ìˆ˜ë™ ì„¤ì •
export LANGSMITH_API_KEY="your-key"
```

### pytest ì‹¤í–‰ ì˜¤ë¥˜
```bash
# ì˜ì¡´ì„± í™•ì¸
pip install pytest langsmith

# Python ê²½ë¡œ í™•ì¸
export PYTHONPATH="${PYTHONPATH}:/path/to/ai/src"
```

## ğŸ“š ì°¸ê³  ìë£Œ

- [LangSmith Evaluation Guide](https://docs.smith.langchain.com/evaluation)
- [LangGraph Testing](https://langchain-ai.github.io/langgraph/how-tos/testing/)
- [ì¸ìˆ˜ì¸ê³„ ë¬¸ì„œ](../../../CLAUDE.md)

## ğŸ‘¥ ê¸°ì—¬

ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë‚˜ í‰ê°€ ë©”íŠ¸ë¦­ì„ ì¶”ê°€í•˜ê³  ì‹¶ë‹¤ë©´:
1. `intervention_dataset.py`ì— ì¼€ì´ìŠ¤ ì¶”ê°€
2. í•„ìš”ì‹œ `prompts.py`ì— í”„ë¡¬í”„íŠ¸ ì¶”ê°€
3. `evaluate_intervention.py`ì— evaluator ì¶”ê°€
4. PR ìƒì„±

---

**Happy Evaluating! ğŸ‰**
